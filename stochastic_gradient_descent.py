from numpy import *
import numpy as np
import matplotlib.pyplot as plt

def file2matrix(path,delimiter):	#先读入contnet，再将其按行分开，再用eval变成list，再转为矩阵类型
	recordlist=[]
	fp=open(path,"rb")
	content=fp.read()
	fp.close()
	rowlist=content.splitlines()	#按行转换为一维表
	recordlist=[map(eval,row.split(delimiter)) for row in rowlist]	#recordlist为list格式的，再将其转化为mat格式的
	return mat(recordlist)

def logistic(wTx):
	return 1.0/(1.0+exp(-wTx))

def drawScatterbyLabel(plt,Input):	#画出散点图
	m,n=shape(Input)
	target=Input[:,-1]
	for i in xrange(m):
		if target[i]==0:
			plt.scatter(Input[i,0],Input[i,1],c='blue',marker='o')
		else:
			plt.scatter(Input[i,0],Input[i,1],c='red',marker='s')

def buildMat(dataSet):
	m,n=shape(dataSet)
	dataMat=zeros((m,n))
	dataMat[:,0]=1
	dataMat[:,1:]=dataSet[:,:-1]
	return dataMat

Input=file2matrix("D:\\Desktop\\MLBook\\chapter05\\testSet.txt","\t")
target=Input[:,-1]
[m,n]=shape(Input)
weightlist=[]

drawScatterbyLabel(plt,Input)

dataMat=buildMat(Input)
#此处是因为X0=1，故而第一列都是1
alpha=0.001
steps=500

weightmat=mat(zeros((steps,n)))

weights=ones(n)

for j in xrange(steps):
	dataIndex=range(m)
	for i in xrange(m):
		alpha=2/(1.0+i+j)+0.0001	#动态修改步长
		randIndex=int(random.uniform(0,len(dataIndex)))
		gradient=sum(dataMat[randIndex]*weights.T)	#每次只取一组数据进行计算
		output=logistic(gradient)
		errors=target[randIndex]-output
		weights=weights+alpha*errors*dataMat[randIndex]
		del(dataIndex[randIndex])
	weightlist.append(weights)

print weights

k=0
for weight in weightlist:
	weightmat[k,:]=weight
	k+=1

X=linspace(0,steps,steps)

fig=plt.figure()
axes1=plt.subplot(211)
axes2=plt.subplot(212)

axes1.plot(X,-weightmat[:,0]/weightmat[:,2],color='blue',linewidth=1,linestyle="-")
axes2.plot(X,-weightmat[:,1]/weightmat[:,2],color='blue',linewidth=1,linestyle="-")
plt.show()

#we repeatedly run through the training set, and each time we encounter a training example, 
#we update the parameters according to the gradient of the error with respect to that single 
#training example only. This algorithm is called stochastic gradient descent (also incremental 
#gradient descent). Whereas batch gradient descent has to scan through the entire training set
#before taking a single step—a costly operation if m is large—stochastic gradient descent can 
#start making progress right away, and continues to make progress with each example it looks at
